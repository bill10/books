{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import deque\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "#import requesocks\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(isbn):\n",
    "    time.sleep(random.random())\n",
    "    #session = requesocks.session()\n",
    "    #session.proxies = {\n",
    "    #'http': 'socks5://127.0.0.1:9050',\n",
    "    #'https': 'socks5://127.0.0.1:9050'}\n",
    "    url = 'http://www.barnesandnoble.com/w/1100101506?ean={}'.format(isbn)\n",
    "    #html = session.get(url)\n",
    "    html = requests.get(url)\n",
    "    soup = bs(html.text)\n",
    "    ids=url.split('/')[-1].split('?')[0]\n",
    "    try:\n",
    "        isbns=soup.find(\"dt\",text=\"ISBN-13:\").findNextSibling(\"dd\").getText()\n",
    "    except:\n",
    "        isbns=''\n",
    "    try:\n",
    "        titles=soup.findAll('h1',{'itemprop':\"name\"})[0].getText()\n",
    "    except:\n",
    "        titles=''\n",
    "    try:\n",
    "        authors=soup.find('span',{'class':\"contributors\"}).find('a').getText()\n",
    "    except:\n",
    "        authors=''\n",
    "    try:\n",
    "        dates=soup.find(\"dt\",text=\"Publication date:\").findNextSibling(\"dd\").getText()\n",
    "    except:\n",
    "        dates=''\n",
    "    try:\n",
    "        editions=soup.find(\"dt\",text=\"Edition description:\").findNextSibling(\"dd\").getText()\n",
    "    except:\n",
    "        editions=''\n",
    "    try:\n",
    "        pages=soup.find(\"dt\",text=\"Pages:\").findNextSibling(\"dd\").getText()\n",
    "    except:\n",
    "        pages=''\n",
    "    try:\n",
    "        saleRanks=soup.find(\"dt\",text=\"Sales rank:\").findNextSibling(\"dd\").getText()\n",
    "    except:\n",
    "        saleRanks=''\n",
    "    edges=[]\n",
    "    try:\n",
    "        recommendationURL='http://www.barnesandnoble.com/includes/bn-recommendation-content.jsp?count=100&fromWL=&title=Customers+Who+Bought+This+Item+Also+Bought&skuId={}&_={}'.format(isbn,ids)\n",
    "        html = requests.get(recommendationURL)\n",
    "        soup = bs(html.text)\n",
    "        for i in soup.find_all('a',{'class':\"book-carousel-link\"}):\n",
    "            u=i.get('href')\n",
    "            edges.append((isbn,u.split('=')[-1]))\n",
    "    except:\n",
    "        pass\n",
    "    return (html.status_code,(ids,isbns,titles,authors,dates,editions,pages,saleRanks,edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isbns=pickle.load(open('../BookCodes/isbn2isbn13.pkl'))\n",
    "querySet = list(set(zip(*isbns)[1]))\n",
    "\n",
    "pool=Pool(30)\n",
    "results=pool.map(scrape,querySet,len(querySet)/30)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "code,data=zip(*results)\n",
    "books=dict(zip(querySet,data))\n",
    "statusCode=dict(zip(querySet,code))\n",
    "\n",
    "with open('../BarnesNobel/scienceBooks.pkl','wb') as outfile:\n",
    "    pickle.dump(books,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "querySet=[i for i in statusCode if statusCode[i]!=200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../BarnesNobel/scienceBooks.txt','w') as outfile:\n",
    "    for i in querySet:\n",
    "        outfile.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following part is replaced by the spark-ec2 scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool=Pool(30)\n",
    "results=pool.map(scrape,querySet,len(querySet)/30)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "code,data=zip(*results)\n",
    "books=dict(zip(querySet,data))\n",
    "statusCode=dict(zip(querySet,code))\n",
    "\n",
    "with open('../BarnesNobel/scienceBooks_ext1.pkl','wb') as outfile:\n",
    "    pickle.dump(dict(zip(querySet,data)),outfile)\n",
    "\n",
    "with open('../BarnesNobel/scienceBooks_resp.pkl','wb') as outfile:\n",
    "    pickle.dump(dict(zip(querySet,code)),outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get results from ec2 and put them back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdata=pickle.load(open('../BarnesNobel/scienceBooks_ext15.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in newdata:\n",
    "    books[i]=newdata[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resp = pickle.load(open('../BarnesNobel/scienceBooks_resp.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in resp:\n",
    "    statusCode[i]=resp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../BarnesNobel/scienceBooks.pkl','wb') as outfile:\n",
    "    pickle.dump(books,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from igraph import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isbn2asin = pickle.load(open('../AmazonBooks/isbn_lookup.pkl'))\n",
    "books = pickle.load(open('../BarnesNobel/scienceBooks.pkl'))\n",
    "isbns=pickle.load(open('../BookCodes/isbn2isbn13.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22469, 172982, 469668, 469669)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emptyBooks=[i for i in books if not books[i][1]]\n",
    "isolates=[i for i in books if not books[i][-1]]\n",
    "len(emptyBooks), len(isolates), len(books), len(isbns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isbn10,isbn13=zip(*isbns)\n",
    "isbn13to10=dict(zip(isbn13,isbn10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isbnConverter(isbn13):\n",
    "    isbn10=isbn13[3:-1]\n",
    "    m=10\n",
    "    check=0\n",
    "    for i in isbn10:\n",
    "        check+=int(i)*m\n",
    "        m-=1\n",
    "    last=(11-(check % 11)) % 11\n",
    "    if last==10:\n",
    "        isbn10=isbn13[3:-1]+'X'\n",
    "        if isbn10 not in isbn2asin:\n",
    "            isbn10=isbn13[3:-1]+'x'\n",
    "    else:\n",
    "        isbn10=isbn13[3:-1]+str(last)\n",
    "    return isbn10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IGRAPH UN-- 439603 3375406 -- \\n+ attr: name (v)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgeList=[(isbn2asin[isbnConverter(e[0])],isbn2asin[isbnConverter(e[1])]) for i in books for e in books[i][-1] if (isbnConverter(e[0]) in isbn2asin and isbnConverter(e[1]) in isbn2asin)]\n",
    "nodes=set([i for e in edgeList for i in e])\n",
    "g = Graph()\n",
    "g.add_vertices(list(nodes))\n",
    "g.add_edges(edgeList)\n",
    "g.simplify()\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asin2id=dict()\n",
    "for u in g.vs:\n",
    "    asin2id[u['name']]=u.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(g,open('../BarnesNobel/asin_network.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfile=open('../BarnesNobel/copurchase_list.txt','w')\n",
    "for e in g.es:\n",
    "    outfile.write('{} {}\\n'.format(g.vs[e.source]['name'],g.vs[e.target]['name']))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(asin2id,open('../BarnesNobel/asin2id.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
